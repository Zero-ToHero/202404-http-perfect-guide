### 웹로봇

- 사람과 상호작용 없이 연속된 웹 트랜잭션을 계속해서 수행하는 HTTP 클라이언트 프로그램의 일종
- 웹 링크를 돌아다니면서 문서를 끌어오고, 이 문서들의 내용을 통해 검색 엔진을 사용할 수 있다.

### 크롤러
- 웹 페이지를 가져오고, 그 페이지들이 가리키는 모든 웹페이지들을 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- 어느 한 웹 링크(루트 집합)에서 시작하여, 그 웹링크가 가리키는 웹 링크를 따라가는 방식
    - 루트 집합(root set) - 크롤러가 방문을 시작하는 URL들의 초기 집합
    - 로봇의 신원 식별 헤더
      - user-agent : 요청을 만든 로봇의 이름
      - From : 사용자, 관리자의 이메일 주소
      - Accept : 서버에게 어떤 미디어 파일을 보내도 되는지
      - Refere : 현재의 요청 URL을 포함한 문서의 URL
     
### 로봇차단하기

- robots.txt
    - 로봇이 어떤 페이지에 접근하기 전에 우선 그 사이트의 robots.txt를 요청하고, 해당 파일에서 권한이 있을 때만 페이지를 가져온다.
    - robots.txt를 요청했을 때, 상태 코드에 따라 다음과 같이 동작한다.
        - 2XX : 로봇은 반드시 그 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려고 할 때 그 규칙을 따른다
        - 404 : 활성화된 차단 규칙이 존재하지 않는 다고 가정하고 제약없이 사이트에 접근
        - 401,403 : 접근제한 → 로봇은 그 사이트로의 접근이 완전히 제한되어 있다고 가정
        - 503 : 일시적 실패 → 그 사이트의 리소스를 검색하는 것을 뒤로 미룬다
        - 3XX : 리다이렉션 → 리소스가 발견될 때까지 리다이렉트를 따라간다
    - user-agent : 로봇의 이름
    - Disallow, Allow : 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지돼 있고, 명시적으로 허용되었는지 기술
    
    ``` json

    User-agent: *
    Disallow: /
    User-agent: Yeti
    Allow: /

    ```

### 검색 엔진의 동작

웹 크롤러가 검색 엔진에게 웹에 존재하는 문서들을 가져다 줌 

→ 검색 엔진은 어떤 문서에 어떤 단어들이 존재하는지 색인을 생성 

→ 사용자가 질의를 보냄

→ 게이트웨이 프로그램이 검색 질의 추출

→ 웹 UI 질의를 풀  텍스트 색인을 검색할 때 사용하는 표현식으로 변환

→ 검색 결과를 정렬 후 보여줌
