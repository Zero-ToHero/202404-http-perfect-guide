# 웹 로봇

- 크롤링 이유 : 필요한 정보를 빠른 시간에 수집하고 데이터 분석이 가능
- 크롤링 : 웹 페이지의 링크를 타고 계속해서 탐색 사용자가 찾는 정보와 연관된 링크
- 스크래핑 : 특정 웹 사이트 탐색, 상품의 가격, 주식 정보, 뉴스 등 원하는 데이터가 명확, 흩어져있는 데이터를 자동으로 추출

## 9.4 로봇 차단하기
- robots.txt 로봇 배제 표준(Robots Exclusion Standard)
- 9.4.2 웹 사이트에 robots.txt파일이 존재한다면 로봇은 반드시 그 파일을 가져와 처리해야 한다.
- 크롤링 자체는 불법이 아니지만 서비스 이용약관을 무시하고 사이트 운영자의 동의 없이 특정 데이터를 크롤링한다면 문제가 될 수 있음.
### 윤리적으로 웹사이트를 크롤링하는 방법
1. Robots.txt 파일 확인하기. 
   - 웹 사이트에 로봇이 무단으로 접근하는 것을 방지하기 위한 프로토콜 
   - 웹 사이트에서 크롤링할 수 있는 범위를 안내해줌 
   - 크롤링하기 전에 웹 사이트의 URL 뒤에 /robots.txt를 덧붙여 해당 URL에 접속하면 robots.txt 파일 내용을 확인해볼 수 있음
2. 이용 약관 충분히 숙지하기
   - 웹 사이트의 콘텐츠를 상업적으로 활용하거나 소유자 혹은 제작자의 동의 없이 무단으로 복제하는 경우 저작권 침해로 간주될 수 있음
   - 저작권이 있거나 개인적인 자료를 동의 없이 상업적인 목적으로 사용하지 않도록 주의 필요. 
   - 특정 정보만을 취사선택해 수집하더라도 법적 문제가 없는지 미리 확인해야 함
3. 크롤링을 하고자 하는 웹 사이트의 서버에 부하 주지 않기. 
   - 웹 사이트에서 데이터를 추출하기 위해 한 번에 너무 많은 요청을 보내면 해당 웹 서버에 과부하 문제가 발생할 수 있음. 
   - 웹 사이트에서 반복적으로 너무 많은 요청이 들어올 경우 로봇으로 간주해 데이터 요청을 차단하는 경우가 많음.

### 네이버 홈 /robots.txt
- 모든 문서에 대해 접근을 차단하고, 첫 페이지에 대해서만 허가
- /.well-known/privacy-sandbox-attestations.json 루트에 위치한 해당 파일만 크롤링 허용
  - Google의 Privacy Sandbox와 관련된 데이터 또는 설정 파일일 가능성이 높음
  - Privacy Sandbox는 사용자의 프라이버시를 보호하면서 웹 광고를 개인화할 수 있도록 돕는 기술
```
User-agent: *     
Disallow: /      
Allow : /$      
Allow : /.well-known/privacy-sandbox-attestations.json
```

### 네이버 웹툰 /robots.txt
- 모든 문서에대한 접근 차단
- 네이버 검색 엔진 클로러인 Yeti 에 한해서 /navercontest/2022/ 경로에 크롤링 허용
```
User-agent: *
Disallow: /

User-agent: Yeti
Disallow: /
Allow: /navercontest/2022/
```

### wirebarley /robots.txt
- 모든 문서에 대한 접근 허용
- api 경로 아래에 모든 파일은 크롤링 불가
```
User-agent: *
Allow: /
Disallow: /api/
```